{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-15T00:52:54.194727300Z",
     "start_time": "2024-04-15T00:52:54.151650900Z"
    }
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Literal\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class DatasetConstructor:\n",
    "    def __init__(self, mode: Literal['train', 'test']):\n",
    "        self.mode = mode\n",
    "        self.path = PATH_PARQUETS / mode\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_memory_usage_pl(df):\n",
    "        \"\"\" Reduce memory usage by polars dataframe {df} with name {name} by changing its data types.\n",
    "            Original pandas version of this function: https://www.kaggle.com/code/arjanso/reducing-dataframe-memory-size-by-65 \"\"\"\n",
    "        print(f\"Memory usage of dataframe is {round(df.estimated_size('mb'), 2)} MB\")\n",
    "        Numeric_Int_types = [pl.Int8, pl.Int16, pl.Int32, pl.Int64]\n",
    "        Numeric_Float_types = [pl.Float32, pl.Float64]\n",
    "        for col in df.columns:\n",
    "            try:\n",
    "                col_type = df[col].dtype\n",
    "                if col_type == pl.Categorical:\n",
    "                    continue\n",
    "                c_min = df[col].min()\n",
    "                c_max = df[col].max()\n",
    "                if col_type in Numeric_Int_types:\n",
    "                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int8))\n",
    "                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int16))\n",
    "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int32))\n",
    "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Int64))\n",
    "                elif col_type in Numeric_Float_types:\n",
    "                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                        df = df.with_columns(df[col].cast(pl.Float32))\n",
    "                    else:\n",
    "                        pass\n",
    "                # elif col_type == pl.Utf8:\n",
    "                #     df = df.with_columns(df[col].cast(pl.Categorical))\n",
    "                else:\n",
    "                    pass\n",
    "            except:\n",
    "                pass\n",
    "        print(f\"Memory usage of dataframe became {round(df.estimated_size('mb'), 2)} MB\")\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def detect_datetime_cols(df):\n",
    "        return df.select_dtypes(object).apply(lambda x: pd.to_datetime(x, errors='ignore'), axis=0).select_dtypes(\n",
    "            np.datetime64).columns.tolist()\n",
    "\n",
    "    def _to_pandas(self, df):\n",
    "        df = df.to_pandas().set_index('case_id')\n",
    "        df = df.replace([np.inf, -np.inf], np.nan)\n",
    "        return df\n",
    "\n",
    "    def merge_static(self, df):\n",
    "        df_static = (\n",
    "            pl.concat(\n",
    "                [pl.scan_parquet(p, low_memory=True) for p in glob.glob(str(self.path / f\"{self.mode}_static_0_*\"))],\n",
    "                how=\"vertical_relaxed\", )\n",
    "            .with_columns(\n",
    "                [\n",
    "                    (pl.col(col).cast(pl.String).str.to_date(strict=False))\n",
    "                    for col in [\n",
    "                    'datefirstoffer_1144D',\n",
    "                    'datelastinstal40dpd_247D',\n",
    "                    'datelastunpaid_3546854D',\n",
    "                    'dtlastpmtallstes_4499206D',\n",
    "                    'firstclxcampaign_1125D',\n",
    "                    'firstdatedue_489D',\n",
    "                    'lastactivateddate_801D',\n",
    "                    'lastapplicationdate_877D',\n",
    "                    'lastapprdate_640D',\n",
    "                    'lastdelinqdate_224D',\n",
    "                    'lastrejectdate_50D',\n",
    "                    'lastrepayingdate_696D',\n",
    "                    'maxdpdinstldate_3546855D',\n",
    "                    'payvacationpostpone_4187118D',\n",
    "                    'validfrom_1069D'\n",
    "                ]\n",
    "                ] + [\n",
    "                    (pl.col(col).cast(pl.String).cast(pl.Categorical))\n",
    "                    for col in [\n",
    "                        'bankacctype_710L', 'cardtype_51L', 'credtype_322L',\n",
    "                        'disbursementtype_67L', 'equalitydataagreement_891L',\n",
    "                        'equalityempfrom_62L', 'inittransactioncode_186L',\n",
    "                        'isbidproductrequest_292L', 'isdebitcard_729L',\n",
    "                        'lastapprcommoditycat_1041M', 'lastapprcommoditytypec_5251766M',\n",
    "                        'lastcancelreason_561M', 'lastrejectcommoditycat_161M',\n",
    "                        'lastrejectcommodtypec_5251769M', 'lastrejectreason_759M',\n",
    "                        'lastrejectreasonclient_4145040M', 'lastst_736L', 'opencred_647L',\n",
    "                        'paytype1st_925L', 'paytype_783L', 'previouscontdistrict_112M',\n",
    "                        'twobodfilling_608L', 'typesuite_864L'\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df.join(df_static, how=\"left\", on=\"case_id\")\n",
    "\n",
    "    def merge_static_cb(self, df):\n",
    "        df_static_cb = (\n",
    "            pl.scan_parquet(self.path / f\"{self.mode}_static_cb_0.parquet\", low_memory=True)\n",
    "            .with_columns(\n",
    "                [\n",
    "                    (pl.col(col).cast(pl.String).str.to_date(strict=False))\n",
    "                    for col in [\n",
    "                    'assignmentdate_238D',\n",
    "                    'assignmentdate_4527235D',\n",
    "                    'assignmentdate_4955616D',\n",
    "                    'birthdate_574D',\n",
    "                    'dateofbirth_337D',\n",
    "                    'dateofbirth_342D',\n",
    "                    'responsedate_1012D',\n",
    "                    'responsedate_4527233D',\n",
    "                    'responsedate_4917613D'\n",
    "                ]\n",
    "                ] + [\n",
    "                    (pl.col(col).cast(pl.String).cast(pl.Categorical))\n",
    "                    for col in [\n",
    "                        'description_5085714M', 'education_1103M', 'education_88M',\n",
    "                        'maritalst_385M', 'maritalst_893M', 'requesttype_4525192L',\n",
    "                        'riskassesment_302T'\n",
    "                    ]\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        return df.join(df_static_cb, how=\"left\", on=\"case_id\")\n",
    "\n",
    "    def load(self):\n",
    "        df = pl.scan_parquet(self.path / f\"{self.mode}_base.parquet\", low_memory=True).with_columns(\n",
    "            pl.col(\"date_decision\").str.to_date()\n",
    "        )\n",
    "        # Depth=0\n",
    "        df = self.merge_static(df)\n",
    "        df = self.merge_static_cb(df)\n",
    "\n",
    "        df = (\n",
    "            df\n",
    "            .with_columns(\n",
    "                pl.col(pl.Float64).cast(pl.Float32),\n",
    "                pl.col(pl.Int64).cast(pl.Int32),\n",
    "            )\n",
    "        )\n",
    "        df = df.select(~cs.date())\n",
    "\n",
    "        # Drop categorical large-dimension columns\n",
    "        df = df.drop([\n",
    "            'lastapprcommoditytypec_5251766M',\n",
    "            'previouscontdistrict_112M',\n",
    "            'district_544M',\n",
    "            'profession_152M',\n",
    "            'name_4527232M',\n",
    "            'name_4917606M',\n",
    "            'employername_160M',\n",
    "            'classificationofcontr_400M',\n",
    "            'financialinstitution_382M',\n",
    "            'contaddr_district_15M',\n",
    "            'contaddr_zipcode_807M',\n",
    "            'empladdr_district_926M',\n",
    "            'empladdr_zipcode_114M',\n",
    "            'registaddr_district_1083M',\n",
    "            'registaddr_zipcode_184M',\n",
    "            'addres_district_368M',\n",
    "            'addres_zip_823M'])\n",
    "        df = df.collect()\n",
    "        df = self.reduce_memory_usage_pl(df)\n",
    "        df = self._to_pandas(df)\n",
    "        return df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-15T00:52:55.912308200Z",
     "start_time": "2024-04-15T00:52:55.886734300Z"
    }
   },
   "id": "ff4accfe9a164104"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_constructor = DatasetConstructor('train')\n",
    "df_train = train_constructor.load()\n",
    "df_train.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80e1359de5a6bd51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_constructor = DatasetConstructor('test')\n",
    "df_test = test_constructor.load()\n",
    "df_test.info()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14a57e561e2fa110"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.drop(columns='target'), df_train['target']\n",
    "X_test,  y_test  = df_test.drop(columns='target'),  df_test['target']\n",
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7294b5fc319f70"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "cf37d02d5da7c6c5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
